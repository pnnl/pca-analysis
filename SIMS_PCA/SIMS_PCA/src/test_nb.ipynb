{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  [25.02673425 25.04207698 18.98346009 12.02303403 25.0341716  24.02093281\n",
      " 12.01476112  2.0172077  17.00094655 16.00578802 16.00135855 17.01666921\n",
      " 24.02365827 13.03376307 18.99872681 16.00354095 12.99321443 24.01442844\n",
      " 18.98913247  2.01823093 13.00657972 17.00125768 25.03548677 16.00465218\n",
      " 19.00741689 13.02029975 26.02199951 12.01393662  1.00933354 16.01135847\n",
      " 12.00161484 15.98267865  2.01856522 24.0206913  25.04168372 12.00979752\n",
      " 26.02745525 25.03040884 24.02355504  2.03085061 18.99404805 25.03650631\n",
      " 17.00717863 26.00528869 26.00741805 26.01232931 13.02457721  1.00334771\n",
      "  1.00757214 12.0112102  25.01543983 25.03839779 12.01296505 26.03039862\n",
      " 18.99639306 17.01338926 18.99552835 18.99085791 18.99762262 13.01536759\n",
      " 25.03052282 12.01109264  1.99623511 15.98973636 25.02956752  2.02679665\n",
      " 19.00654154 26.02519478 17.01866149  2.00726363 12.00825392 15.99665323\n",
      " 24.02983943 18.98681599 16.0063513  17.0103751  25.04028688 13.01003242\n",
      "  0.99542513  1.00708217 17.00855242 24.03480535 17.01701393  1.00373007\n",
      " 25.04441516 19.01169279 17.00372674  2.03106397 24.01867624 16.00906835\n",
      " 13.00031948 18.98901486  1.99240708 16.01243923  1.01460489 25.04115635\n",
      " 16.99645225 16.01201033 11.99307219 26.03375307]\n",
      "y_train:  ['C2H', 'C2H', 'F-', 'C-', 'C2H', 'C2-', 'C-', '2H-', 'OH-', 'O-', 'O-', 'OH-', 'C2-', 'CH-', 'F-', 'O-', 'CH-', 'C2-', 'F-', '2H-', 'CH-', 'OH-', 'C2H', 'O-', 'F-', 'CH-', 'CN-', 'C-', 'H-', 'O-', 'C-', 'O-', '2H-', 'C2-', 'C2H', 'C-', 'CN-', 'C2H', 'C2-', '2H-', 'F-', 'C2H', 'OH-', 'CN-', 'CN-', 'CN-', 'CH-', 'H-', 'H-', 'C-', 'C2H', 'C2H', 'C-', 'CN-', 'F-', 'OH-', 'F-', 'F-', 'F-', 'CH-', 'C2H', 'C-', '2H-', 'O-', 'C2H', '2H-', 'F-', 'CN-', 'OH-', '2H-', 'C-', 'O-', 'C2-', 'F-', 'O-', 'OH-', 'C2H', 'CH-', 'H-', 'H-', 'OH-', 'C2-', 'OH-', 'H-', 'C2H', 'F-', 'OH-', '2H-', 'C2-', 'O-', 'CH-', 'F-', '2H-', 'O-', 'H-', 'C2H', 'OH-', 'O-', 'C-', 'CN-']\n"
     ]
    }
   ],
   "source": [
    "# Generate training data. We use a compound lookup table, then add some noise.\n",
    "data = pd.read_csv(\"train_data.csv\")\n",
    "\n",
    "# Select a bunch of random rows in the data\n",
    "np.random.seed(2)\n",
    "masses = []\n",
    "species = []\n",
    "for i in range(100):\n",
    "    rand_index = np.random.randint(10)\n",
    "    masses.append(data.at[rand_index, \"Precise Mass\"])\n",
    "    species.append(data.at[rand_index, \"Species\"])\n",
    "masses = np.array(masses)\n",
    "\n",
    "# Generate noise for the same number of rows'\n",
    "noise = 0.01 * np.random.standard_normal(size = 100)\n",
    "\n",
    "# Combine the noise and randomly selected masses to create our training data\n",
    "x_train = masses + noise\n",
    "y_train = species\n",
    "print(\"x_train: \", x_train)\n",
    "print(\"y_train: \", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO\n",
    "# Normalize data before we pass it through our network.\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(mass_data.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our model.\n",
    "class speciesClassification(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim): # Feel free to add hidden_dim as parameters here\n",
    "        \n",
    "        # TODO\n",
    "        super(speciesClassification, self).__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # Create input layer, 2 hidden layers of 32 neurons each with ReLu activation functions after each layer, and\n",
    "        # output layer.\n",
    "        self.layer1 = nn.Linear(input_dim, 32)\n",
    "        self.actfun1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.actfun2 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(32, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # Send our inputs through the network defined above.\n",
    "        x = self.layer1(x)\n",
    "        x = self.actfun1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.actfun2(x)\n",
    "        x = self.output_layer(x)\n",
    "        out = self.softmax(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our neural network model with input and output dimensions. Inputs are images with 28 x 28 pixels, output is \n",
    "# a digit from 0-9.\n",
    "model = speciesClassification(28**2, 10)\n",
    "\n",
    "# Define the learning rate and epoch \n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "batchsize = 50\n",
    "\n",
    "# Define loss function and optimizer. Start out using SGD (TODO: CHANGE LATER IF NEEDED).\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Run this line if you have PyTorch GPU version\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for training loss and validation accuracy during training\n",
    "# Training loss should be tracked for each iteration (1 iteration -> single forward pass to the network)\n",
    "# Validation accuracy should be evaluated every 'Epoch' (1 epoch -> full training dataset)\n",
    "# If using batch gradient, 1 iteration = 1 epoch\n",
    "\n",
    "train_loss_list = []\n",
    "validation_accuracy_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "# Convert the training, validation, testing dataset (NumPy arrays) into torch tensors\n",
    "\n",
    "# YOUR CODE HERE\n",
    "x_train_final = torch.from_numpy(x_train).float()\n",
    "y_train_final = torch.from_numpy(y_train).long()\n",
    "\n",
    "validation_inputs = torch.from_numpy(x_validation).float()\n",
    "validation_targets = torch.from_numpy(y_validation).long()\n",
    "\n",
    "testing_inputs = torch.from_numpy(x_test).float()\n",
    "testing_targets = torch.from_numpy(y_test).long()\n",
    "\n",
    "# Training Loop ---------------------------------------------------------------------------------------\n",
    "\n",
    "for epoch in tqdm.trange(epochs):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Step through the optimizer and update the loss on each step.\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x_train_final)\n",
    "\n",
    "    # print(outputs.shape)\n",
    "    # print(y_train_final.shape)\n",
    "    \n",
    "    loss = loss_func(outputs, y_train_final)\n",
    "    train_loss_list.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Compute Validation Accuracy ----------------------------------------------------------------------\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    with torch.no_grad(): # Tell PyTorch that we aren't passing inputs to network for training purpose\n",
    "        \n",
    "        # Pass the validation feature data (30 samples) to the network\n",
    "        validation_outputs = model(validation_inputs)\n",
    "        \n",
    "        # validation_outputs = tensor of shape (30, 3), i.e., 30 predictions and each prediction has 3 probabilities.\n",
    "        # These are classification probabilities for each flower type: [p(setosa), p(versicolor), p(virginica)].\n",
    "        # torch.argmax(validation_outputs, dim=1) finds the index with the maximum value alongside the column direction\n",
    "        # i.e. for each sample, it finds the column index with the highest probability.\n",
    "        # == validation_targets compares these indices with groundtruth validation target labels for each sample\n",
    "        # For each sample, it returns True if the index matches the target, False otherwise\n",
    "        # .type(torch.FloatTensor) converts True = 1, False = 0\n",
    "        # Finally .mean() gives us (Total count of 1) / (Length of the 1D tensor), giving us the classification accuracy  \n",
    "        \n",
    "        correct = (torch.argmax(validation_outputs, dim=1) == \n",
    "                   validation_targets).type(torch.FloatTensor)\n",
    "        \n",
    "        validation_accuracy_list.append(correct.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import seaborn for prettier plots\n",
    "\n",
    "import seaborn as sns\n",
    "# Visualize training loss\n",
    "\n",
    "plt.figure(figsize = (12, 6))\n",
    "\n",
    "# Visualize training loss with respect to iterations (1 iteration -> single batch)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_loss_list, linewidth = 3)\n",
    "plt.ylabel(\"training loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "sns.despine()\n",
    "\n",
    "# Visualize validation accuracy with respect to epochs\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(validation_accuracy_list, linewidth = 3, color = 'gold')\n",
    "plt.ylabel(\"validation accuracy\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Compute the testing accuracy.\n",
    "with torch.no_grad(): # Tell PyTorch that we aren't passing inputs to network for training purpose.\n",
    "\n",
    "    # Pass the testing feature data to the network to produce model predictions.\n",
    "    y_pred_test = model(testing_inputs) \n",
    "    \n",
    "    # Use the same technique as above to compute the testing classification accuracy.\n",
    "    correct = (torch.argmax(y_pred_test, dim=1) == testing_targets).type(torch.FloatTensor)\n",
    "    \n",
    "    # Print test accuracy to the user.\n",
    "    print(\"Testing Accuracy: \" + str(correct.mean().numpy()*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions of our prediction and actual arrays and print some useful things.\n",
    "print(torch.argmax(y_pred_test, dim=1))\n",
    "print(testing_inputs.numpy().shape)\n",
    "print(y_pred_test.numpy().shape)\n",
    "print(correct.numpy().shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
